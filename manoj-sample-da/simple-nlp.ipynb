{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "178021d6-1e52-476b-a10f-379ff30e85ee",
   "metadata": {},
   "source": [
    "Install the required libraries from nlp-requirements.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c292da7f-6146-4e2e-b9fb-999e6b744b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.3\n",
      "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorflow==2.7.0\n",
      "  Downloading tensorflow-2.7.0-cp38-cp38-manylinux2010_x86_64.whl (489.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.6/489.6 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn==0.24.1 in /opt/app-root/lib/python3.8/site-packages (from -r nlp-requirements.txt (line 4)) (0.24.1)\n",
      "Requirement already satisfied: numpy==1.19.2 in /opt/app-root/lib/python3.8/site-packages (from -r nlp-requirements.txt (line 5)) (1.19.2)\n",
      "Requirement already satisfied: pandas==1.2.4 in /opt/app-root/lib/python3.8/site-packages (from -r nlp-requirements.txt (line 6)) (1.2.4)\n",
      "Requirement already satisfied: six in /opt/app-root/lib/python3.8/site-packages (from nltk==3.3->-r nlp-requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (1.1.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (1.32.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (2.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (0.2.0)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 kB\u001b[0m \u001b[31m301.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (1.12.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (1.6.3)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m299.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (0.27.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (3.7.4.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (0.15.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (0.3.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (0.38.1)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (2.6.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (14.0.6)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/app-root/lib/python3.8/site-packages (from tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (3.20.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/app-root/lib/python3.8/site-packages (from scikit-learn==0.24.1->-r nlp-requirements.txt (line 4)) (1.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/app-root/lib/python3.8/site-packages (from scikit-learn==0.24.1->-r nlp-requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/app-root/lib/python3.8/site-packages (from scikit-learn==0.24.1->-r nlp-requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/app-root/lib/python3.8/site-packages (from pandas==1.2.4->-r nlp-requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/app-root/lib/python3.8/site-packages (from pandas==1.2.4->-r nlp-requirements.txt (line 6)) (2022.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (1.8.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (65.5.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (1.35.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/app-root/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/app-root/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/app-root/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (5.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/app-root/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/app-root/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (1.26.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/app-root/lib/python3.8/site-packages (from werkzeug>=0.11.15->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/app-root/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (3.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/app-root/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/app-root/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r nlp-requirements.txt (line 3)) (3.2.2)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394470 sha256=786098113340c49b34a65d879f256ca084d3a360cf3e8b812600572b9e2a6fd9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wkd6ig55/wheels/b5/0e/53/5cfbc5d26eca272bf9e6694836529448e7724765b6ae040d79\n",
      "Successfully built nltk\n",
      "Installing collected packages: tensorflow-estimator, keras, nltk, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.4.0\n",
      "    Uninstalling tensorflow-estimator-2.4.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.4.3\n",
      "    Uninstalling Keras-2.4.3:\n",
      "      Successfully uninstalled Keras-2.4.3\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.4.4\n",
      "    Uninstalling tensorflow-2.4.4:\n",
      "      Successfully uninstalled tensorflow-2.4.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-gpu 2.7.4 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.7.0 nltk-3.3 tensorflow-2.7.0 tensorflow-estimator-2.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r nlp-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f9d9a3d-9d1b-4c67-b098-7defccc6b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df: pd.DataFrame = pd.read_csv('dataset/simple_conv.csv')\n",
    "train_df1: pd.DataFrame = pd.read_csv('dataset/chat-sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9319500-1c85-4e0e-8b65-b24bde6fa42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  um, how are you guys?\n",
       "1      How is Chris and how are you &=distortion? \u001554...\n",
       "2      Well pretty well Chris is not, is not here at ...\n",
       "3      She's been away with her class up in North . \u0015...\n",
       "4                                    oh. \u0015552110_552570\u0015\n",
       "                             ...                        \n",
       "356    because &uh you showed us her picture. \u0015112942...\n",
       "357                  yeah yeah Crazy . \u00151130960_1133380\u0015\n",
       "358    um and I keep thinking maybe I'd get in touch ...\n",
       "359    and then I saw in the &um Tufts thing that Gin...\n",
       "360                             yeah . \u00151142710_1143070\u0015\n",
       "Name: Conversation, Length: 361, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"Conversation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "707bf124-d511-4e8b-879b-d3d4c4b3f0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /opt/app-\n",
      "[nltk_data]     root/src/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /opt/app-\n",
      "[nltk_data]     root/src/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to /opt/app-\n",
      "[nltk_data]     root/src/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    "    \n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad144895-3dde-460a-98a8-f4a22aa16c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf136635-fb67-40f7-9d5e-848f206d6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [s for s in train_df[\"Conversation\"]]\n",
    "\n",
    "words = [w for s in sentences for w in nltk.word_tokenize(s)  if w.lower() not in stopwords]\n",
    "\n",
    "manual_cleanup = [',', '?', '&', '.' ]\n",
    "\n",
    "words = [w for w in words if w not in manual_cleanup]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c879ed9-0313-4238-af73-b104eb11d819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yeah', 61),\n",
       " (\"'s\", 59),\n",
       " ('um', 43),\n",
       " ('know', 30),\n",
       " (\"n't\", 24),\n",
       " ('uh', 17),\n",
       " ('oh', 15),\n",
       " (\"'re\", 13),\n",
       " ('Well', 11),\n",
       " ('got', 11)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(words)\n",
    "\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4eb27665-6a41-4804-b8a7-52bd1bcee9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((\"n't\", 'know'), 6),\n",
       " (('oh', 'yeah'), 4),\n",
       " (('passed', 'away'), 4),\n",
       " (('know', \"'s\"), 4),\n",
       " (('@', 'sdeu'), 4)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finder = nltk.collocations.TrigramCollocationFinder.from_words(words)\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "#finder = nltk.collocations.QuadgramCollocationFinder.from_words(words)\n",
    "\n",
    "\n",
    "finder.ngram_fd.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64b19097-9c21-402d-ba0b-6b24cd865e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.549, 'pos': 0.451, 'compound': 0.7506}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "sia.polarity_scores(sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d5d0f58-0efe-47ea-9bd0-89c6e5657b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well the relative scene is kind of sad.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f22226e4-db1b-4759-868c-c830a7659f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.295, 'neu': 0.522, 'pos': 0.183, 'compound': -0.3167}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VadarSentimentIntensityAnalyzer\n",
    "\n",
    "siav = VadarSentimentIntensityAnalyzer()\n",
    "\n",
    "sia.polarity_scores(sentences[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b9d05-2a90-4813-a073-2a0d43cfe20f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
